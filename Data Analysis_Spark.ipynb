{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8b3ba45-9b0b-43f2-9c84-2ce8e7aec6ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "For help, look here:\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0145c6ca-d9f7-4bd7-8309-3fee0a69c3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check out the pre-loaded dataset\n",
    "display(dbutils.fs.ls('dbfs:/databricks-datasets/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195d300a-63f0-4dc8-b0c4-bc1c4cf6a9b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51aa0658-e68f-4a62-8962-5b1baa0ede76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a rdd (sc = SparkContext)\n",
    "rdd = spark.read.text(\"dbfs:/databricks-datasets/SPARK_README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaf066b0-5738-4d2d-a225-8ae0d44d3a78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: [Row(value='# Apache Spark'),\n Row(value=''),\n Row(value='Spark is a fast and general cluster computing system for Big Data. It provides'),\n Row(value='high-level APIs in Scala, Java, Python, and R, and an optimized engine that'),\n Row(value='supports general computation graphs for data analysis. It also supports a'),\n Row(value='rich set of higher-level tools including Spark SQL for SQL and DataFrames,'),\n Row(value='MLlib for machine learning, GraphX for graph processing,'),\n Row(value='and Spark Streaming for stream processing.'),\n Row(value=''),\n Row(value='<http://spark.apache.org/>'),\n Row(value=''),\n Row(value=''),\n Row(value='## Online Documentation'),\n Row(value=''),\n Row(value='You can find the latest Spark documentation, including a programming'),\n Row(value='guide, on the [project web page](http://spark.apache.org/documentation.html)'),\n Row(value='and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).'),\n Row(value='This README file only contains basic setup instructions.'),\n Row(value=''),\n Row(value='## Building Spark')]"
     ]
    }
   ],
   "source": [
    "# Read 20 lines \n",
    "rdd.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ceadc51-a08e-4d51-8f18-7470d8f4ed06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(word='#')\nRow(word='Apache')\nRow(word='Spark')\nRow(word='')\nRow(word='Spark')\nRow(word='is')\nRow(word='a')\nRow(word='fast')\nRow(word='and')\nRow(word='general')\nRow(word='cluster')\nRow(word='computing')\nRow(word='system')\nRow(word='for')\nRow(word='Big')\nRow(word='Data.')\nRow(word='It')\nRow(word='provides')\nRow(word='high-level')\nRow(word='APIs')\nRow(word='in')\nRow(word='Scala,')\nRow(word='Java,')\nRow(word='Python,')\nRow(word='and')\nRow(word='R,')\nRow(word='and')\nRow(word='an')\nRow(word='optimized')\nRow(word='engine')\nRow(word='that')\nRow(word='supports')\nRow(word='general')\nRow(word='computation')\nRow(word='graphs')\nRow(word='for')\nRow(word='data')\nRow(word='analysis.')\nRow(word='It')\nRow(word='also')\nRow(word='supports')\nRow(word='a')\nRow(word='rich')\nRow(word='set')\nRow(word='of')\nRow(word='higher-level')\nRow(word='tools')\nRow(word='including')\nRow(word='Spark')\nRow(word='SQL')\nRow(word='for')\nRow(word='SQL')\nRow(word='and')\nRow(word='DataFrames,')\nRow(word='MLlib')\nRow(word='for')\nRow(word='machine')\nRow(word='learning,')\nRow(word='GraphX')\nRow(word='for')\nRow(word='graph')\nRow(word='processing,')\nRow(word='and')\nRow(word='Spark')\nRow(word='Streaming')\nRow(word='for')\nRow(word='stream')\nRow(word='processing.')\nRow(word='')\nRow(word='<http://spark.apache.org/>')\nRow(word='')\nRow(word='')\nRow(word='##')\nRow(word='Online')\nRow(word='Documentation')\nRow(word='')\nRow(word='You')\nRow(word='can')\nRow(word='find')\nRow(word='the')\nRow(word='latest')\nRow(word='Spark')\nRow(word='documentation,')\nRow(word='including')\nRow(word='a')\nRow(word='programming')\nRow(word='guide,')\nRow(word='on')\nRow(word='the')\nRow(word='[project')\nRow(word='web')\nRow(word='page](http://spark.apache.org/documentation.html)')\nRow(word='and')\nRow(word='[project')\nRow(word='wiki](https://cwiki.apache.org/confluence/display/SPARK).')\nRow(word='This')\nRow(word='README')\nRow(word='file')\nRow(word='only')\nRow(word='contains')\nRow(word='basic')\nRow(word='setup')\nRow(word='instructions.')\nRow(word='')\nRow(word='##')\nRow(word='Building')\nRow(word='Spark')\nRow(word='')\nRow(word='Spark')\nRow(word='is')\nRow(word='built')\nRow(word='using')\nRow(word='[Apache')\nRow(word='Maven](http://maven.apache.org/).')\nRow(word='To')\nRow(word='build')\nRow(word='Spark')\nRow(word='and')\nRow(word='its')\nRow(word='example')\nRow(word='programs,')\nRow(word='run:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='build/mvn')\nRow(word='-DskipTests')\nRow(word='clean')\nRow(word='package')\nRow(word='')\nRow(word='(You')\nRow(word='do')\nRow(word='not')\nRow(word='need')\nRow(word='to')\nRow(word='do')\nRow(word='this')\nRow(word='if')\nRow(word='you')\nRow(word='downloaded')\nRow(word='a')\nRow(word='pre-built')\nRow(word='package.)')\nRow(word='More')\nRow(word='detailed')\nRow(word='documentation')\nRow(word='is')\nRow(word='available')\nRow(word='from')\nRow(word='the')\nRow(word='project')\nRow(word='site,')\nRow(word='at')\nRow(word='[\"Building')\nRow(word='Spark\"](http://spark.apache.org/docs/latest/building-spark.html).')\nRow(word='')\nRow(word='##')\nRow(word='Interactive')\nRow(word='Scala')\nRow(word='Shell')\nRow(word='')\nRow(word='The')\nRow(word='easiest')\nRow(word='way')\nRow(word='to')\nRow(word='start')\nRow(word='using')\nRow(word='Spark')\nRow(word='is')\nRow(word='through')\nRow(word='the')\nRow(word='Scala')\nRow(word='shell:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='./bin/spark-shell')\nRow(word='')\nRow(word='Try')\nRow(word='the')\nRow(word='following')\nRow(word='command,')\nRow(word='which')\nRow(word='should')\nRow(word='return')\nRow(word='1000:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='scala>')\nRow(word='sc.parallelize(1')\nRow(word='to')\nRow(word='1000).count()')\nRow(word='')\nRow(word='##')\nRow(word='Interactive')\nRow(word='Python')\nRow(word='Shell')\nRow(word='')\nRow(word='Alternatively,')\nRow(word='if')\nRow(word='you')\nRow(word='prefer')\nRow(word='Python,')\nRow(word='you')\nRow(word='can')\nRow(word='use')\nRow(word='the')\nRow(word='Python')\nRow(word='shell:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='./bin/pyspark')\nRow(word='')\nRow(word='And')\nRow(word='run')\nRow(word='the')\nRow(word='following')\nRow(word='command,')\nRow(word='which')\nRow(word='should')\nRow(word='also')\nRow(word='return')\nRow(word='1000:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='>>>')\nRow(word='sc.parallelize(range(1000)).count()')\nRow(word='')\nRow(word='##')\nRow(word='Example')\nRow(word='Programs')\nRow(word='')\nRow(word='Spark')\nRow(word='also')\nRow(word='comes')\nRow(word='with')\nRow(word='several')\nRow(word='sample')\nRow(word='programs')\nRow(word='in')\nRow(word='the')\nRow(word='`examples`')\nRow(word='directory.')\nRow(word='To')\nRow(word='run')\nRow(word='one')\nRow(word='of')\nRow(word='them,')\nRow(word='use')\nRow(word='`./bin/run-example')\nRow(word='<class>')\nRow(word='[params]`.')\nRow(word='For')\nRow(word='example:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='./bin/run-example')\nRow(word='SparkPi')\nRow(word='')\nRow(word='will')\nRow(word='run')\nRow(word='the')\nRow(word='Pi')\nRow(word='example')\nRow(word='locally.')\nRow(word='')\nRow(word='You')\nRow(word='can')\nRow(word='set')\nRow(word='the')\nRow(word='MASTER')\nRow(word='environment')\nRow(word='variable')\nRow(word='when')\nRow(word='running')\nRow(word='examples')\nRow(word='to')\nRow(word='submit')\nRow(word='examples')\nRow(word='to')\nRow(word='a')\nRow(word='cluster.')\nRow(word='This')\nRow(word='can')\nRow(word='be')\nRow(word='a')\nRow(word='mesos://')\nRow(word='or')\nRow(word='spark://')\nRow(word='URL,')\nRow(word='\"yarn\"')\nRow(word='to')\nRow(word='run')\nRow(word='on')\nRow(word='YARN,')\nRow(word='and')\nRow(word='\"local\"')\nRow(word='to')\nRow(word='run')\nRow(word='locally')\nRow(word='with')\nRow(word='one')\nRow(word='thread,')\nRow(word='or')\nRow(word='\"local[N]\"')\nRow(word='to')\nRow(word='run')\nRow(word='locally')\nRow(word='with')\nRow(word='N')\nRow(word='threads.')\nRow(word='You')\nRow(word='can')\nRow(word='also')\nRow(word='use')\nRow(word='an')\nRow(word='abbreviated')\nRow(word='class')\nRow(word='name')\nRow(word='if')\nRow(word='the')\nRow(word='class')\nRow(word='is')\nRow(word='in')\nRow(word='the')\nRow(word='`examples`')\nRow(word='package.')\nRow(word='For')\nRow(word='instance:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='MASTER=spark://host:7077')\nRow(word='./bin/run-example')\nRow(word='SparkPi')\nRow(word='')\nRow(word='Many')\nRow(word='of')\nRow(word='the')\nRow(word='example')\nRow(word='programs')\nRow(word='print')\nRow(word='usage')\nRow(word='help')\nRow(word='if')\nRow(word='no')\nRow(word='params')\nRow(word='are')\nRow(word='given.')\nRow(word='')\nRow(word='##')\nRow(word='Running')\nRow(word='Tests')\nRow(word='')\nRow(word='Testing')\nRow(word='first')\nRow(word='requires')\nRow(word='[building')\nRow(word='Spark](#building-spark).')\nRow(word='Once')\nRow(word='Spark')\nRow(word='is')\nRow(word='built,')\nRow(word='tests')\nRow(word='can')\nRow(word='be')\nRow(word='run')\nRow(word='using:')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='')\nRow(word='./dev/run-tests')\nRow(word='')\nRow(word='Please')\nRow(word='see')\nRow(word='the')\nRow(word='guidance')\nRow(word='on')\nRow(word='how')\nRow(word='to')\nRow(word='[run')\nRow(word='tests')\nRow(word='for')\nRow(word='a')\nRow(word='module,')\nRow(word='or')\nRow(word='individual')\nRow(word='tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).')\nRow(word='')\nRow(word='##')\nRow(word='A')\nRow(word='Note')\nRow(word='About')\nRow(word='Hadoop')\nRow(word='Versions')\nRow(word='')\nRow(word='Spark')\nRow(word='uses')\nRow(word='the')\nRow(word='Hadoop')\nRow(word='core')\nRow(word='library')\nRow(word='to')\nRow(word='talk')\nRow(word='to')\nRow(word='HDFS')\nRow(word='and')\nRow(word='other')\nRow(word='Hadoop-supported')\nRow(word='storage')\nRow(word='systems.')\nRow(word='Because')\nRow(word='the')\nRow(word='protocols')\nRow(word='have')\nRow(word='changed')\nRow(word='in')\nRow(word='different')\nRow(word='versions')\nRow(word='of')\nRow(word='Hadoop,')\nRow(word='you')\nRow(word='must')\nRow(word='build')\nRow(word='Spark')\nRow(word='against')\nRow(word='the')\nRow(word='same')\nRow(word='version')\nRow(word='that')\nRow(word='your')\nRow(word='cluster')\nRow(word='runs.')\nRow(word='')\nRow(word='Please')\nRow(word='refer')\nRow(word='to')\nRow(word='the')\nRow(word='build')\nRow(word='documentation')\nRow(word='at')\nRow(word='[\"Specifying')\nRow(word='the')\nRow(word='Hadoop')\nRow(word='Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)')\nRow(word='for')\nRow(word='detailed')\nRow(word='guidance')\nRow(word='on')\nRow(word='building')\nRow(word='for')\nRow(word='a')\nRow(word='particular')\nRow(word='distribution')\nRow(word='of')\nRow(word='Hadoop,')\nRow(word='including')\nRow(word='building')\nRow(word='for')\nRow(word='particular')\nRow(word='Hive')\nRow(word='and')\nRow(word='Hive')\nRow(word='Thriftserver')\nRow(word='distributions.')\nRow(word='')\nRow(word='##')\nRow(word='Configuration')\nRow(word='')\nRow(word='Please')\nRow(word='refer')\nRow(word='to')\nRow(word='the')\nRow(word='[Configuration')\nRow(word='Guide](http://spark.apache.org/docs/latest/configuration.html)')\nRow(word='in')\nRow(word='the')\nRow(word='online')\nRow(word='documentation')\nRow(word='for')\nRow(word='an')\nRow(word='overview')\nRow(word='on')\nRow(word='how')\nRow(word='to')\nRow(word='configure')\nRow(word='Spark.')\n"
     ]
    }
   ],
   "source": [
    "%python\n",
    "words = rdd.selectExpr(\"explode(split(value, ' ')) as word\")\n",
    "\n",
    "for w in words.collect():\n",
    "  print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "222ddcdd-271e-4703-9cd9-e2ed3bdf92aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\nApache\nSpark\n\nSpark\nis\na\nfast\nand\ngeneral\ncluster\ncomputing\nsystem\nfor\nBig\nData.\nIt\nprovides\nhigh-level\nAPIs\nin\nScala,\nJava,\nPython,\nand\nR,\nand\nan\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nIt\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nSpark\nSQL\nfor\nSQL\nand\nDataFrames,\nMLlib\nfor\nmachine\nlearning,\nGraphX\nfor\ngraph\nprocessing,\nand\nSpark\nStreaming\nfor\nstream\nprocessing.\n\n<http://spark.apache.org/>\n\n\n##\nOnline\nDocumentation\n\nYou\ncan\nfind\nthe\nlatest\nSpark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\nthe\n[project\nweb\npage](http://spark.apache.org/documentation.html)\nand\n[project\nwiki](https://cwiki.apache.org/confluence/display/SPARK).\nThis\nREADME\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nBuilding\nSpark\n\nSpark\nis\nbuilt\nusing\n[Apache\nMaven](http://maven.apache.org/).\nTo\nbuild\nSpark\nand\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-DskipTests\nclean\npackage\n\n(You\ndo\nnot\nneed\nto\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nMore\ndetailed\ndocumentation\nis\navailable\nfrom\nthe\nproject\nsite,\nat\n[\"Building\nSpark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\nInteractive\nScala\nShell\n\nThe\neasiest\nway\nto\nstart\nusing\nSpark\nis\nthrough\nthe\nScala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\nTry\nthe\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala>\nsc.parallelize(1\nto\n1000).count()\n\n##\nInteractive\nPython\nShell\n\nAlternatively,\nif\nyou\nprefer\nPython,\nyou\ncan\nuse\nthe\nPython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nAnd\nrun\nthe\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n>>>\nsc.parallelize(range(1000)).count()\n\n##\nExample\nPrograms\n\nSpark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nin\nthe\n`examples`\ndirectory.\nTo\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nFor\nexample:\n\n\n\n\n\n./bin/run-example\nSparkPi\n\nwill\nrun\nthe\nPi\nexample\nlocally.\n\nYou\ncan\nset\nthe\nMASTER\nenvironment\nvariable\nwhen\nrunning\nexamples\nto\nsubmit\nexamples\nto\na\ncluster.\nThis\ncan\nbe\na\nmesos://\nor\nspark://\nURL,\n\"yarn\"\nto\nrun\non\nYARN,\nand\n\"local\"\nto\nrun\nlocally\nwith\none\nthread,\nor\n\"local[N]\"\nto\nrun\nlocally\nwith\nN\nthreads.\nYou\ncan\nalso\nuse\nan\nabbreviated\nclass\nname\nif\nthe\nclass\nis\nin\nthe\n`examples`\npackage.\nFor\ninstance:\n\n\n\n\n\nMASTER=spark://host:7077\n./bin/run-example\nSparkPi\n\nMany\nof\nthe\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nRunning\nTests\n\nTesting\nfirst\nrequires\n[building\nSpark](#building-spark).\nOnce\nSpark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nPlease\nsee\nthe\nguidance\non\nhow\nto\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\n\n##\nA\nNote\nAbout\nHadoop\nVersions\n\nSpark\nuses\nthe\nHadoop\ncore\nlibrary\nto\ntalk\nto\nHDFS\nand\nother\nHadoop-supported\nstorage\nsystems.\nBecause\nthe\nprotocols\nhave\nchanged\nin\ndifferent\nversions\nof\nHadoop,\nyou\nmust\nbuild\nSpark\nagainst\nthe\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nPlease\nrefer\nto\nthe\nbuild\ndocumentation\nat\n[\"Specifying\nthe\nHadoop\nVersion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nHadoop,\nincluding\nbuilding\nfor\nparticular\nHive\nand\nHive\nThriftserver\ndistributions.\n\n##\nConfiguration\n\nPlease\nrefer\nto\nthe\n[Configuration\nGuide](http://spark.apache.org/docs/latest/configuration.html)\nin\nthe\nonline\ndocumentation\nfor\nan\noverview\non\nhow\nto\nconfigure\nSpark.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "words_df = rdd.select(explode(split(col(\"value\"), \" \")).alias(\"word\"))\n",
    "\n",
    "for w in words_df.collect():\n",
    "    print(w['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91d9a83d-2ecd-4f59-80b6-dae5646d91c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#', 'apache', 'spark']\n['']\n['spark', 'is', 'a', 'fast', 'and', 'general', 'cluster', 'computing', 'system', 'for', 'big', 'data.', 'it', 'provides']\n['high-level', 'apis', 'in', 'scala,', 'java,', 'python,', 'and', 'r,', 'and', 'an', 'optimized', 'engine', 'that']\n['supports', 'general', 'computation', 'graphs', 'for', 'data', 'analysis.', 'it', 'also', 'supports', 'a']\n['rich', 'set', 'of', 'higher-level', 'tools', 'including', 'spark', 'sql', 'for', 'sql', 'and', 'dataframes,']\n['mllib', 'for', 'machine', 'learning,', 'graphx', 'for', 'graph', 'processing,']\n['and', 'spark', 'streaming', 'for', 'stream', 'processing.']\n['']\n['<http://spark.apache.org/>']\n['']\n['']\n['##', 'online', 'documentation']\n['']\n['you', 'can', 'find', 'the', 'latest', 'spark', 'documentation,', 'including', 'a', 'programming']\n['guide,', 'on', 'the', '[project', 'web', 'page](http://spark.apache.org/documentation.html)']\n['and', '[project', 'wiki](https://cwiki.apache.org/confluence/display/spark).']\n['this', 'readme', 'file', 'only', 'contains', 'basic', 'setup', 'instructions.']\n['']\n['##', 'building', 'spark']\n['']\n['spark', 'is', 'built', 'using', '[apache', 'maven](http://maven.apache.org/).']\n['to', 'build', 'spark', 'and', 'its', 'example', 'programs,', 'run:']\n['']\n['', '', '', '', 'build/mvn', '-dskiptests', 'clean', 'package']\n['']\n['(you', 'do', 'not', 'need', 'to', 'do', 'this', 'if', 'you', 'downloaded', 'a', 'pre-built', 'package.)']\n['more', 'detailed', 'documentation', 'is', 'available', 'from', 'the', 'project', 'site,', 'at']\n['[\"building', 'spark\"](http://spark.apache.org/docs/latest/building-spark.html).']\n['']\n['##', 'interactive', 'scala', 'shell']\n['']\n['the', 'easiest', 'way', 'to', 'start', 'using', 'spark', 'is', 'through', 'the', 'scala', 'shell:']\n['']\n['', '', '', '', './bin/spark-shell']\n['']\n['try', 'the', 'following', 'command,', 'which', 'should', 'return', '1000:']\n['']\n['', '', '', '', 'scala>', 'sc.parallelize(1', 'to', '1000).count()']\n['']\n['##', 'interactive', 'python', 'shell']\n['']\n['alternatively,', 'if', 'you', 'prefer', 'python,', 'you', 'can', 'use', 'the', 'python', 'shell:']\n['']\n['', '', '', '', './bin/pyspark']\n['']\n['and', 'run', 'the', 'following', 'command,', 'which', 'should', 'also', 'return', '1000:']\n['']\n['', '', '', '', '>>>', 'sc.parallelize(range(1000)).count()']\n['']\n['##', 'example', 'programs']\n['']\n['spark', 'also', 'comes', 'with', 'several', 'sample', 'programs', 'in', 'the', '`examples`', 'directory.']\n['to', 'run', 'one', 'of', 'them,', 'use', '`./bin/run-example', '<class>', '[params]`.', 'for', 'example:']\n['']\n['', '', '', '', './bin/run-example', 'sparkpi']\n['']\n['will', 'run', 'the', 'pi', 'example', 'locally.']\n['']\n['you', 'can', 'set', 'the', 'master', 'environment', 'variable', 'when', 'running', 'examples', 'to', 'submit']\n['examples', 'to', 'a', 'cluster.', 'this', 'can', 'be', 'a', 'mesos://', 'or', 'spark://', 'url,']\n['\"yarn\"', 'to', 'run', 'on', 'yarn,', 'and', '\"local\"', 'to', 'run']\n['locally', 'with', 'one', 'thread,', 'or', '\"local[n]\"', 'to', 'run', 'locally', 'with', 'n', 'threads.', 'you']\n['can', 'also', 'use', 'an', 'abbreviated', 'class', 'name', 'if', 'the', 'class', 'is', 'in', 'the', '`examples`']\n['package.', 'for', 'instance:']\n['']\n['', '', '', '', 'master=spark://host:7077', './bin/run-example', 'sparkpi']\n['']\n['many', 'of', 'the', 'example', 'programs', 'print', 'usage', 'help', 'if', 'no', 'params', 'are', 'given.']\n['']\n['##', 'running', 'tests']\n['']\n['testing', 'first', 'requires', '[building', 'spark](#building-spark).', 'once', 'spark', 'is', 'built,', 'tests']\n['can', 'be', 'run', 'using:']\n['']\n['', '', '', '', './dev/run-tests']\n['']\n['please', 'see', 'the', 'guidance', 'on', 'how', 'to']\n['[run', 'tests', 'for', 'a', 'module,', 'or', 'individual', 'tests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).']\n['']\n['##', 'a', 'note', 'about', 'hadoop', 'versions']\n['']\n['spark', 'uses', 'the', 'hadoop', 'core', 'library', 'to', 'talk', 'to', 'hdfs', 'and', 'other', 'hadoop-supported']\n['storage', 'systems.', 'because', 'the', 'protocols', 'have', 'changed', 'in', 'different', 'versions', 'of']\n['hadoop,', 'you', 'must', 'build', 'spark', 'against', 'the', 'same', 'version', 'that', 'your', 'cluster', 'runs.']\n['']\n['please', 'refer', 'to', 'the', 'build', 'documentation', 'at']\n['[\"specifying', 'the', 'hadoop', 'version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)']\n['for', 'detailed', 'guidance', 'on', 'building', 'for', 'a', 'particular', 'distribution', 'of', 'hadoop,', 'including']\n['building', 'for', 'particular', 'hive', 'and', 'hive', 'thriftserver', 'distributions.']\n['']\n['##', 'configuration']\n['']\n['please', 'refer', 'to', 'the', '[configuration', 'guide](http://spark.apache.org/docs/latest/configuration.html)']\n['in', 'the', 'online', 'documentation', 'for', 'an', 'overview', 'on', 'how', 'to', 'configure', 'spark.']\n"
     ]
    }
   ],
   "source": [
    "# 2. change all capital letters to lower case\n",
    "from pyspark.sql.functions import explode, split, col, lower\n",
    "words_df = rdd.select(split(lower(col(\"value\")), ' ').alias(\"word\"))\n",
    "\n",
    "for w in words_df.collect():\n",
    "    print(w['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a41d715-efd1-48be-8c6d-f822e71fb4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\napache\nspark\n\nspark\nis\na\nfast\ngeneral\ncluster\ncomputing\nsystem\nfor\nbig\ndata.\nit\nprovides\nhigh-level\napis\nscala,\njava,\npython,\nr,\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis.\nit\nalso\nsupports\na\nrich\nset\nof\nhigher-level\ntools\nincluding\nspark\nsql\nfor\nsql\ndataframes,\nmllib\nfor\nmachine\nlearning,\ngraphx\nfor\ngraph\nprocessing,\nspark\nstreaming\nfor\nstream\nprocessing.\n\n<http://spark.apache.org/>\n\n\n##\nonline\ndocumentation\n\nyou\ncan\nfind\nlatest\nspark\ndocumentation,\nincluding\na\nprogramming\nguide,\non\n[project\nweb\npage](http://spark.apache.org/documentation.html)\n[project\nwiki](https://cwiki.apache.org/confluence/display/spark).\nthis\nreadme\nfile\nonly\ncontains\nbasic\nsetup\ninstructions.\n\n##\nbuilding\nspark\n\nspark\nis\nbuilt\nusing\n[apache\nmaven](http://maven.apache.org/).\nbuild\nspark\nits\nexample\nprograms,\nrun:\n\n\n\n\n\nbuild/mvn\n-dskiptests\nclean\npackage\n\n(you\ndo\nnot\nneed\ndo\nthis\nif\nyou\ndownloaded\na\npre-built\npackage.)\nmore\ndetailed\ndocumentation\nis\navailable\nfrom\nproject\nsite,\n[\"building\nspark\"](http://spark.apache.org/docs/latest/building-spark.html).\n\n##\ninteractive\nscala\nshell\n\neasiest\nway\nstart\nusing\nspark\nis\nthrough\nscala\nshell:\n\n\n\n\n\n./bin/spark-shell\n\ntry\nfollowing\ncommand,\nwhich\nshould\nreturn\n1000:\n\n\n\n\n\nscala>\nsc.parallelize(1\n1000).count()\n\n##\ninteractive\npython\nshell\n\nalternatively,\nif\nyou\nprefer\npython,\nyou\ncan\nuse\npython\nshell:\n\n\n\n\n\n./bin/pyspark\n\nrun\nfollowing\ncommand,\nwhich\nshould\nalso\nreturn\n1000:\n\n\n\n\n\n>>>\nsc.parallelize(range(1000)).count()\n\n##\nexample\nprograms\n\nspark\nalso\ncomes\nwith\nseveral\nsample\nprograms\n`examples`\ndirectory.\nrun\none\nof\nthem,\nuse\n`./bin/run-example\n<class>\n[params]`.\nfor\nexample:\n\n\n\n\n\n./bin/run-example\nsparkpi\n\nwill\nrun\npi\nexample\nlocally.\n\nyou\ncan\nset\nmaster\nenvironment\nvariable\nwhen\nrunning\nexamples\nsubmit\nexamples\na\ncluster.\nthis\ncan\nbe\na\nmesos://\nor\nspark://\nurl,\n\"yarn\"\nrun\non\nyarn,\n\"local\"\nrun\nlocally\nwith\none\nthread,\nor\n\"local[n]\"\nrun\nlocally\nwith\nn\nthreads.\nyou\ncan\nalso\nuse\nabbreviated\nclass\nname\nif\nclass\nis\n`examples`\npackage.\nfor\ninstance:\n\n\n\n\n\nmaster=spark://host:7077\n./bin/run-example\nsparkpi\n\nmany\nof\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven.\n\n##\nrunning\ntests\n\ntesting\nfirst\nrequires\n[building\nspark](#building-spark).\nonce\nspark\nis\nbuilt,\ntests\ncan\nbe\nrun\nusing:\n\n\n\n\n\n./dev/run-tests\n\nplease\nsee\nguidance\non\nhow\n[run\ntests\nfor\na\nmodule,\nor\nindividual\ntests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).\n\n##\na\nnote\nabout\nhadoop\nversions\n\nspark\nuses\nhadoop\ncore\nlibrary\ntalk\nhdfs\nother\nhadoop-supported\nstorage\nsystems.\nbecause\nprotocols\nhave\nchanged\ndifferent\nversions\nof\nhadoop,\nyou\nmust\nbuild\nspark\nagainst\nsame\nversion\nthat\nyour\ncluster\nruns.\n\nplease\nrefer\nbuild\ndocumentation\n[\"specifying\nhadoop\nversion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nhadoop,\nincluding\nbuilding\nfor\nparticular\nhive\nhive\nthriftserver\ndistributions.\n\n##\nconfiguration\n\nplease\nrefer\n[configuration\nguide](http://spark.apache.org/docs/latest/configuration.html)\nonline\ndocumentation\nfor\noverview\non\nhow\nconfigure\nspark.\n"
     ]
    }
   ],
   "source": [
    "# 3. eliminate stopwords \n",
    "words_df = rdd.select(explode(split(lower(col(\"value\")), ' ')).alias(\"word\"))\n",
    "\n",
    "# 3. eliminate stopwords\n",
    "stop_words = ['and', 'to', 'in', 'at', 'the', 'an']\n",
    "filtered_words_df = words_df.filter(~col(\"word\").isin(stop_words))\n",
    "\n",
    "for w in filtered_words_df.collect():\n",
    "    print(w['word'])\n",
    "stop_words = ['and', 'to', 'in', 'at', 'the', 'an']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b3b435-df3e-4181-8eb4-453cc354e576",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\"local\"\n\"local[n]\"\n\"yarn\"\n#\n##\n##\n##\n##\n##\n##\n##\n##\n(you\n-dskiptests\n./bin/pyspark\n./bin/run-example\n./bin/run-example\n./bin/spark-shell\n./dev/run-tests\n1000).count()\n1000:\n1000:\n<class>\n<http://spark.apache.org/>\n>>>\n[\"building\n[\"specifying\n[apache\n[building\n[configuration\n[params]`.\n[project\n[project\n[run\n`./bin/run-example\n`examples`\n`examples`\na\na\na\na\na\na\na\na\na\nabbreviated\nabout\nagainst\nalso\nalso\nalso\nalso\nalternatively,\nanalysis.\napache\napis\nare\navailable\nbasic\nbe\nbe\nbecause\nbig\nbuild\nbuild\nbuild\nbuild/mvn\nbuilding\nbuilding\nbuilding\nbuilt\nbuilt,\ncan\ncan\ncan\ncan\ncan\ncan\nchanged\nclass\nclass\nclean\ncluster\ncluster\ncluster.\ncomes\ncommand,\ncommand,\ncomputation\ncomputing\nconfiguration\nconfigure\ncontains\ncore\ndata\ndata.\ndataframes,\ndetailed\ndetailed\ndifferent\ndirectory.\ndistribution\ndistributions.\ndo\ndo\ndocumentation\ndocumentation\ndocumentation\ndocumentation\ndocumentation,\ndownloaded\neasiest\nengine\nenvironment\nexample\nexample\nexample\nexample\nexample:\nexamples\nexamples\nfast\nfile\nfind\nfirst\nfollowing\nfollowing\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfor\nfrom\ngeneral\ngeneral\ngiven.\ngraph\ngraphs\ngraphx\nguidance\nguidance\nguide,\nguide](http://spark.apache.org/docs/latest/configuration.html)\nhadoop\nhadoop\nhadoop\nhadoop,\nhadoop,\nhadoop-supported\nhave\nhdfs\nhelp\nhigh-level\nhigher-level\nhive\nhive\nhow\nhow\nif\nif\nif\nif\nincluding\nincluding\nincluding\nindividual\ninstance:\ninstructions.\ninteractive\ninteractive\nis\nis\nis\nis\nis\nis\nit\nit\nits\njava,\nlatest\nlearning,\nlibrary\nlocally\nlocally\nlocally.\nmachine\nmany\nmaster\nmaster=spark://host:7077\nmaven](http://maven.apache.org/).\nmesos://\nmllib\nmodule,\nmore\nmust\nn\nname\nneed\nno\nnot\nnote\nof\nof\nof\nof\nof\non\non\non\non\non\nonce\none\none\nonline\nonline\nonly\noptimized\nor\nor\nor\nother\noverview\npackage\npackage.\npackage.)\npage](http://spark.apache.org/documentation.html)\nparams\nparticular\nparticular\npi\nplease\nplease\nplease\npre-built\nprefer\nprint\nprocessing,\nprocessing.\nprogramming\nprograms\nprograms\nprograms\nprograms,\nproject\nprotocols\nprovides\npython\npython\npython,\npython,\nr,\nreadme\nrefer\nrefer\nrequires\nreturn\nreturn\nrich\nrun\nrun\nrun\nrun\nrun\nrun\nrun\nrun:\nrunning\nrunning\nruns.\nsame\nsample\nsc.parallelize(1\nsc.parallelize(range(1000)).count()\nscala\nscala\nscala,\nscala>\nsee\nset\nset\nsetup\nseveral\nshell\nshell\nshell:\nshell:\nshould\nshould\nsite,\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\nspark\"](http://spark.apache.org/docs/latest/building-spark.html).\nspark.\nspark://\nspark](#building-spark).\nsparkpi\nsparkpi\nsql\nsql\nstart\nstorage\nstream\nstreaming\nsubmit\nsupports\nsupports\nsystem\nsystems.\ntalk\ntesting\ntests\ntests\ntests\ntests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools).\nthat\nthat\nthem,\nthis\nthis\nthis\nthread,\nthreads.\nthriftserver\nthrough\ntools\ntry\nurl,\nusage\nuse\nuse\nuse\nuses\nusing\nusing\nusing:\nvariable\nversion\nversion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\nversions\nversions\nway\nweb\nwhen\nwhich\nwhich\nwiki](https://cwiki.apache.org/confluence/display/spark).\nwill\nwith\nwith\nwith\nyarn,\nyou\nyou\nyou\nyou\nyou\nyou\nyou\nyour\n"
     ]
    }
   ],
   "source": [
    "# 4. sort in alphabetical order\n",
    "sorted_words_df = filtered_words_df.orderBy(\"word\")\n",
    "\n",
    "for w in sorted_words_df.collect():\n",
    "    print(w['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "103ffb91-f715-4b9d-b6ac-8c47fb699233",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphs 1\npi 1\nabbreviated 1\noverview 1\nrich 1\nurl, 1\nname 1\nstream 1\nrun: 1\nnot 1\nguide](http://spark.apache.org/docs/latest/configuration.html) 1\n./dev/run-tests 1\nwill 1\n[run 1\nbecause 1\nmust 1\nmaster=spark://host:7077 1\nvariable 1\ncore 1\ngraphx 1\nmore 1\n[configuration 1\nprotocols 1\njava, 1\nsite, 1\nsystems. 1\n[building 1\nconfigure 1\nalternatively, 1\nsystem 1\nprovides 1\npre-built 1\ndirectory. 1\napis 1\ndata. 1\nwiki](https://cwiki.apache.org/confluence/display/spark). 1\nlibrary 1\ncontains 1\nprogramming 1\ndownloaded 1\n1000).count() 1\ncomes 1\nmachine 1\nparams 1\nn 1\ngiven. 1\nsame 1\npage](http://spark.apache.org/documentation.html) 1\nusing: 1\nfast 1\nstreaming 1\nyour 1\noptimized 1\ngraph 1\npackage 1\nmaster 1\nproject 1\nother 1\nlearning, 1\nwhen 1\nsubmit 1\nversion\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version) 1\nscala> 1\nprint 1\ntesting 1\ndifferent 1\nspark. 1\nconfiguration 1\napache 1\ndata 1\n-dskiptests 1\nprocessing. 1\nmaven](http://maven.apache.org/). 1\nits 1\nbasic 1\nlatest 1\ntests](https://cwiki.apache.org/confluence/display/spark/useful+developer+tools). 1\nonly 1\n<class> 1\nhave 1\nruns. 1\n\"local\" 1\nprocessing, 1\nbuilt 1\nthread, 1\nfile 1\ncomputation 1\nnote 1\ndataframes, 1\nfind 1\n(you 1\nsc.parallelize(1 1\nuses 1\nprograms, 1\n\"yarn\" 1\nsee 1\nthriftserver 1\n./bin/pyspark 1\ncomputing 1\nmllib 1\nfrom 1\nanalysis. 1\ncluster. 1\ntalk 1\ndistributions. 1\nguide, 1\nr, 1\nsetup 1\nneed 1\nspark:// 1\nhadoop-supported 1\nare 1\nrequires 1\npackage. 1\nclean 1\nsc.parallelize(range(1000)).count() 1\nhigh-level 1\nagainst 1\nthrough 1\npackage.) 1\neasiest 1\nno 1\nseveral 1\nhelp 1\nonce 1\nsample 1\nspark](#building-spark). 1\n# 1\nusage 1\nway 1\ntry 1\nprefer 1\nbuild/mvn 1\nweb 1\nlocally. 1\nspark\"](http://spark.apache.org/docs/latest/building-spark.html). 1\n[\"building 1\nhdfs 1\nhigher-level 1\ntools 1\navailable 1\nabout 1\n>>> 1\nreadme 1\nscala, 1\n<http://spark.apache.org/> 1\nenvironment 1\nbuilt, 1\nmodule, 1\nthem, 1\n`./bin/run-example 1\ninstance: 1\nfirst 1\ndocumentation, 1\n[params]`. 1\nmesos:// 1\nengine 1\nexample: 1\nmany 1\n[apache 1\nyarn, 1\nindividual 1\nchanged 1\n./bin/spark-shell 1\nthreads. 1\nstorage 1\nversion 1\ninstructions. 1\nstart 1\n\"local[n]\" 1\n[\"specifying 1\nbig 1\ndistribution 1\nonline 2\ncommand, 2\nset 2\nparticular 2\nusing 2\nguidance 2\nshell: 2\nhow 2\none 2\nbe 2\nlocally 2\nhadoop, 2\npython, 2\nshould 2\n[project 2\n`examples` 2\nversions 2\ngeneral 2\n1000: 2\ndetailed 2\nsql 2\nscala 2\nfollowing 2\nrefer 2\n./bin/run-example 2\nit 2\nreturn 2\ncluster 2\nshell 2\ninteractive 2\ndo 2\nclass 2\nexamples 2\nthat 2\nrunning 2\nsparkpi 2\npython 2\nsupports 2\nwhich 2\nhive 2\nprograms 3\ntests 3\nplease 3\nwith 3\nbuilding 3\nuse 3\nbuild 3\nincluding 3\nthis 3\nhadoop 3\nor 3\ndocumentation 4\nexample 4\nif 4\nalso 4\non 5\nof 5\ncan 6\nis 6\nyou 7\nrun 7\n## 8\na 9\nfor 13\nspark 13\n 67\n"
     ]
    }
   ],
   "source": [
    "# 5. sort from most to least frequent word\n",
    "word_counts_df = filtered_words_df.groupBy(\"word\").count()\n",
    "\n",
    "# Sort from most to least frequent word\n",
    "sorted_word_counts_df = word_counts_df.orderBy((\"count\"))\n",
    "\n",
    "for w in sorted_word_counts_df.collect():\n",
    "    print(w['word'], w['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4eaa9bb-2882-41fc-b676-2c24844e80bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\napache\nspark\n\nspark\nis\na\nfast\ngeneral\ncluster\ncomputing\nsystem\nfor\nbig\ndata\nit\nprovides\nhighlevel\napis\nscala\njava\npython\nr\noptimized\nengine\nthat\nsupports\ngeneral\ncomputation\ngraphs\nfor\ndata\nanalysis\nit\nalso\nsupports\na\nrich\nset\nof\nhigherlevel\ntools\nincluding\nspark\nsql\nfor\nsql\ndataframes\nmllib\nfor\nmachine\nlearning\ngraphx\nfor\ngraph\nprocessing\nspark\nstreaming\nfor\nstream\nprocessing\n\nhttpsparkapacheorg\n\n\n\nonline\ndocumentation\n\nyou\ncan\nfind\nlatest\nspark\ndocumentation\nincluding\na\nprogramming\nguide\non\nproject\nweb\npagehttpsparkapacheorgdocumentationhtml\nproject\nwikihttpscwikiapacheorgconfluencedisplayspark\nthis\nreadme\nfile\nonly\ncontains\nbasic\nsetup\ninstructions\n\n\nbuilding\nspark\n\nspark\nis\nbuilt\nusing\napache\nmavenhttpmavenapacheorg\nbuild\nspark\nits\nexample\nprograms\nrun\n\n\n\n\n\nbuildmvn\ndskiptests\nclean\npackage\n\nyou\ndo\nnot\nneed\ndo\nthis\nif\nyou\ndownloaded\na\nprebuilt\npackage\nmore\ndetailed\ndocumentation\nis\navailable\nfrom\nproject\nsite\nbuilding\nsparkhttpsparkapacheorgdocslatestbuildingsparkhtml\n\n\ninteractive\nscala\nshell\n\neasiest\nway\nstart\nusing\nspark\nis\nthrough\nscala\nshell\n\n\n\n\n\nbinsparkshell\n\ntry\nfollowing\ncommand\nwhich\nshould\nreturn\n1000\n\n\n\n\n\nscala\nscparallelize1\n1000count\n\n\ninteractive\npython\nshell\n\nalternatively\nif\nyou\nprefer\npython\nyou\ncan\nuse\npython\nshell\n\n\n\n\n\nbinpyspark\n\nrun\nfollowing\ncommand\nwhich\nshould\nalso\nreturn\n1000\n\n\n\n\n\n\nscparallelizerange1000count\n\n\nexample\nprograms\n\nspark\nalso\ncomes\nwith\nseveral\nsample\nprograms\nexamples\ndirectory\nrun\none\nof\nthem\nuse\nbinrunexample\nclass\nparams\nfor\nexample\n\n\n\n\n\nbinrunexample\nsparkpi\n\nwill\nrun\npi\nexample\nlocally\n\nyou\ncan\nset\nmaster\nenvironment\nvariable\nwhen\nrunning\nexamples\nsubmit\nexamples\na\ncluster\nthis\ncan\nbe\na\nmesos\nor\nspark\nurl\nyarn\nrun\non\nyarn\nlocal\nrun\nlocally\nwith\none\nthread\nor\nlocaln\nrun\nlocally\nwith\nn\nthreads\nyou\ncan\nalso\nuse\nabbreviated\nclass\nname\nif\nclass\nis\nexamples\npackage\nfor\ninstance\n\n\n\n\n\nmastersparkhost7077\nbinrunexample\nsparkpi\n\nmany\nof\nexample\nprograms\nprint\nusage\nhelp\nif\nno\nparams\nare\ngiven\n\n\nrunning\ntests\n\ntesting\nfirst\nrequires\nbuilding\nsparkbuildingspark\nonce\nspark\nis\nbuilt\ntests\ncan\nbe\nrun\nusing\n\n\n\n\n\ndevruntests\n\nplease\nsee\nguidance\non\nhow\nrun\ntests\nfor\na\nmodule\nor\nindividual\ntestshttpscwikiapacheorgconfluencedisplaysparkusefuldevelopertools\n\n\na\nnote\nabout\nhadoop\nversions\n\nspark\nuses\nhadoop\ncore\nlibrary\ntalk\nhdfs\nother\nhadoopsupported\nstorage\nsystems\nbecause\nprotocols\nhave\nchanged\ndifferent\nversions\nof\nhadoop\nyou\nmust\nbuild\nspark\nagainst\nsame\nversion\nthat\nyour\ncluster\nruns\n\nplease\nrefer\nbuild\ndocumentation\nspecifying\nhadoop\nversionhttpsparkapacheorgdocslatestbuildingsparkhtmlspecifyingthehadoopversion\nfor\ndetailed\nguidance\non\nbuilding\nfor\na\nparticular\ndistribution\nof\nhadoop\nincluding\nbuilding\nfor\nparticular\nhive\nhive\nthriftserver\ndistributions\n\n\nconfiguration\n\nplease\nrefer\nconfiguration\nguidehttpsparkapacheorgdocslatestconfigurationhtml\nonline\ndocumentation\nfor\noverview\non\nhow\nconfigure\nspark\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "\n",
    "# 6.** remove punctuations \n",
    "cleaned_words_df = filtered_words_df.withColumn(\"word\", regexp_replace(col(\"word\"), \"[^\\w\\s]\", \"\"))\n",
    "\n",
    "for w in cleaned_words_df.collect():\n",
    "    print(w['word'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e683e60a-c072-4ace-8ceb-0f615bb9bbcc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. What does it do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "764898e3-0cb4-41c2-8a9c-4d3a712d43bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>average_age</th></tr></thead><tbody><tr><td>Denny</td><td>31.0</td></tr><tr><td>Jules</td><td>30.0</td></tr><tr><td>TD</td><td>35.0</td></tr><tr><td>Brooke</td><td>25.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Denny",
         31.0
        ],
        [
         "Jules",
         30.0
        ],
        [
         "TD",
         35.0
        ],
        [
         "Brooke",
         25.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "average_age",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%python\n",
    "# Create a DataFrame from the data\n",
    "data = [(\"Denny\", 31), (\"Jules\", 30), (\"TD\", 35), (\"Brooke\", 25)]\n",
    "columns = [\"name\", \"age\"]\n",
    "dataDF = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Calculate the average age\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "agesDF = dataDF.groupBy(\"name\").agg(F.avg(\"age\").alias(\"average_age\"))\n",
    "\n",
    "display(agesDF)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "latest",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}